import os
import faiss
import pickle
import openai
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import re
import snowballstemmer
from zemberek_client import get_lemmas

# ===========================
# Load API key
# ===========================
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_TOKEN")

# ===========================
# Load FAISS + metadata
# ===========================
print("üîç Loading FAISS index and metadata...")
index = faiss.read_index("data/vector/disease_faiss.index")

with open("data/vector/disease_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)  # {"texts": [...], "diseases": [...], "departments": [...]}

# ===========================
# Load embedding model
# ===========================
print("üß† Loading embedding model...")
embedding_model = SentenceTransformer("intfloat/multilingual-e5-base")

# ===========================
# Normalization
# ===========================
stemmer = snowballstemmer.stemmer('turkish')
# Example Turkish stopwords list (you can expand this)
TURKISH_STOPWORDS = {
    "ve", "ile", "mi", "da", "de", "bir", "bu", "≈üu", "o", "i√ßin", "ama", "fakat",
    "veya", "√ßok", "gibi", "kadar", "eƒüer", "ise", "daha", "en", "ve", "ki"
}

def normalize_tokens(text):
    """
    Normalize Turkish text for token overlap:
    - Lowercase
    - Remove punctuation and numbers
    - Remove stopwords
    - Lemmatize using Zemberek
    Returns a set of normalized tokens.
    """
    # Step 1: Lemmatize with Zemberek
    lemmas = get_lemmas(text)
    
    normalized_tokens = set()
    for lemma in lemmas:
        # Step 2: Lowercase
        token = lemma.lower()
        # Step 3: Remove punctuation and numbers
        token = re.sub(r'[^a-zƒ±ƒü√º≈ü√∂√ß]', '', token)
        # Step 4: Skip stopwords and empty tokens
        if token and token not in TURKISH_STOPWORDS:
            normalized_tokens.add(token)
    
    return normalized_tokens

def token_overlap(query, doc_text):
    """Compute normalized token overlap."""
    query_tokens = normalize_tokens(query)
    doc_tokens = normalize_tokens(doc_text)

    # Print original texts
    print(f"Query: {query}")
    print(f"Doc text: {doc_text}")
    
    print(f"Query tokens: {query_tokens}")
    print(f"Doc tokens: {doc_tokens}")

    return len(query_tokens & doc_tokens) / max(len(query_tokens), 1)

# ===========================
# Retrieve relevant context
# ===========================
def retrieve_relevant_context(query, k=5):
    query_emb = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_emb, k)

    retrieved = []
    for idx, dist in zip(indices[0], distances[0]):
        i = int(idx)
        if i < len(metadata["texts"]):
            doc_text = metadata["texts"][i]
            similarity = 1 / (1 + dist)
            overlap_score = token_overlap(query, doc_text)
            # Hybrid score: 70% semantic, 30% token overlap
            final_score = 0.7 * similarity + 0.3 * overlap_score

            retrieved.append({
                "text": doc_text,
                "Disease": metadata["diseases"][i],
                "Department": metadata["departments"][i],
                "similarity": similarity,
                "overlap": overlap_score,
                "final_score": final_score
            })

    retrieved = sorted(retrieved, key=lambda x: x["final_score"], reverse=True)
    return retrieved[:k]

def format_context(docs):
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(
            f"{i}. Hastalƒ±k: {doc['Disease']}\n"
            f"B√∂l√ºm: {doc['Department']}\n"
            f"Belirtiler: {doc['text']}\n"
            f"Relevance (semantic similarity): {doc['similarity']:.3f}, "
            f"Token overlap: {doc['overlap']:.3f}, "
            f"Final score: {doc['final_score']:.3f}"
        )
    return "\n".join(formatted)

# ===========================
# Ask GPT-4
# ===========================
def ask_gpt4(user_input):
    patient_symptoms = list(normalize_tokens(user_input))
    retrieved_docs = retrieve_relevant_context(user_input, k=5)
    context_text = format_context(retrieved_docs)

    system_prompt = (
        "Sen bir tƒ±bbi NLP sistemisin. "
        "A≈üaƒüƒ±daki 'veri tabanƒ± i√ßeriƒüi' hastalƒ±k, b√∂l√ºm ve belirtiler bilgisini i√ßerir. "
        "Kullanƒ±cƒ± T√ºrk√ße olarak belirtilerini girecektir. "
        "Yanƒ±tƒ±nƒ± **mutlaka JSON formatƒ±nda ver** ve ba≈üka hi√ßbir metin ekleme. "
        "JSON yapƒ±sƒ± ≈üu ≈üekilde olmalƒ±dƒ±r: "
        "{"
        "'patient_symptoms': [ ... ], "
        "'departments': [ ... ], "
        "'extra_symptoms': { 'Departman Adƒ±': [ ... ], 'Hastalƒ±k Adƒ±': [ ... ] }, "
        "'disease_probabilities': [{ 'disease': 'Hastalƒ±k Adƒ±', 'probability': 0.xx }], "
        "'explanation': '...' "
        "}"
        "Kurallar: "
        "1. 'patient_symptoms' alanƒ±nda, normalize edilmi≈ü kullanƒ±cƒ± belirtilerini listele. "
        "2. Eƒüer belirtiler tek bir departmanla y√ºksek g√ºvenle e≈üle≈üiyorsa, 'departments' listesinde sadece o departmanƒ± ver. "
        "3. Eƒüer belirtiler birden fazla departmanla benzer d√ºzeyde e≈üle≈üiyorsa, 'departments' listesinde en ilgili departmanlarƒ± ver ve "
        "her departman i√ßin 'extra_symptoms' listesinde kullanƒ±cƒ±ya sorulabilecek ek semptomlarƒ± ekle. "
        "4. 'disease_probabilities' alanƒ±nda, **verilen context i√ßinde bulunan T√úM olasƒ± hastalƒ±klarƒ±** (√∂rneƒüin top-k = 5 veya 10), "
        "departman e≈üle≈ümesinden veya olasƒ±lƒ±k d√ºzeyinden baƒüƒ±msƒ±z ≈üekilde **tam liste olarak** ver. "
        "Her hastalƒ±k i√ßin 0.00‚Äì0.99 aralƒ±ƒüƒ±nda makul bir olasƒ±lƒ±k deƒüeri ile doldur (Bunun i√ßin Final score'larƒ± kullan), "
        "ve hi√ßbir hastalƒ±ƒüƒ± atlama. "
        "5. 'extra_symptoms' alanƒ±nda, **departmanlardan baƒüƒ±msƒ±z olarak**, t√ºm hastalƒ±klar ('disease_probabilities'te bulunan) i√ßin kullanƒ±cƒ±ya sorulabilecek √∂nemli semptomlarƒ± ekle. "
        "6. 'explanation' alanƒ±nda doktorun okuyacaƒüƒ± kƒ±sa ama detaylƒ± a√ßƒ±klama olmalƒ±; her hastalƒ±k i√ßin hangi ek semptomlarƒ± dikkate almasƒ± gerektiƒüini belirt. "
        "7. Eƒüer belirtiler context ile e≈üle≈ümiyorsa, 'departments' ve 'disease_probabilities' bo≈ü listeler, 'extra_symptoms' bo≈ü obje, 'explanation' kƒ±sa uyarƒ± mesajƒ± olsun."
    )

    user_prompt = f"Veri tabanƒ± kayƒ±tlarƒ±:\n{context_text}\n\nKullanƒ±cƒ±nƒ±n belirtileri: {user_input}"

    print("\n==================== SYSTEM PROMPT ====================")
    print(system_prompt)
    print("\n==================== USER PROMPT ====================")
    print(user_prompt[:2000])

    response = openai.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
            {
                "role": "user",
                "content": f"Hastanƒ±n belirtileri (tokenizasyon ile √ßƒ±karƒ±lmƒ±≈ü): {patient_symptoms}"
            },
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content, retrieved_docs

# ===========================
# Run example
# ===========================
if __name__ == "__main__":
    print("\nü§ñ RAG-based Disease Prediction System\n")
    user_input = "Ba≈üƒ±m aƒürƒ±yor ve midem bulanƒ±yor"

    answer, docs = ask_gpt4(user_input)

    print("\n==================== AI YANITI ====================")
    print(answer)
