import os
import faiss
import pickle
import openai
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import re
import snowballstemmer
from zemberek_client import get_lemmas

# ===========================
# Load API key
# ===========================
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

openai.api_key = os.getenv("OPENAI_API_TOKEN")

# ===========================
# Load FAISS + metadata
# ===========================
print("ğŸ” Loading FAISS index and metadata...")
# Get the project root directory (two levels up from this file)
project_root = os.path.join(os.path.dirname(__file__), '..', '..')
faiss_index_path = os.path.join(project_root, "data", "vector", "disease_faiss.index")
metadata_path = os.path.join(project_root, "data", "vector", "disease_metadata.pkl")

index = faiss.read_index(faiss_index_path)

with open(metadata_path, "rb") as f:
    metadata = pickle.load(f)  # {"texts": [...], "diseases": [...], "departments": [...]}

# ===========================
# Load embedding model
# ===========================
print("ğŸ§  Loading embedding model...")
embedding_model = SentenceTransformer("intfloat/multilingual-e5-base")

# ===========================
# Normalization
# ===========================
stemmer = snowballstemmer.stemmer('turkish')
# Example Turkish stopwords list (you can expand this)
TURKISH_STOPWORDS = {
    "ve", "ile", "mi", "da", "de", "bir", "bu", "ÅŸu", "o", "iÃ§in", "ama", "fakat",
    "veya", "Ã§ok", "gibi", "kadar", "eÄŸer", "ise", "daha", "en", "ve", "ki"
}

def normalize_tokens(text):
    """
    Normalize Turkish text for token overlap:
    - Lowercase
    - Remove punctuation and numbers
    - Remove stopwords
    - Lemmatize using Zemberek
    Returns a set of normalized tokens.
    """
    # Step 1: Lemmatize with Zemberek
    lemmas = get_lemmas(text)
    
    normalized_tokens = set()
    for lemma in lemmas:
        # Step 2: Lowercase
        token = lemma.lower()
        # Step 3: Remove punctuation and numbers
        token = re.sub(r'[^a-zÄ±ÄŸÃ¼ÅŸÃ¶Ã§]', '', token)
        # Step 4: Skip stopwords and empty tokens
        if token and token not in TURKISH_STOPWORDS:
            normalized_tokens.add(token)
    
    return normalized_tokens

def token_overlap(query, doc_text):
    """Compute normalized token overlap."""
    query_tokens = normalize_tokens(query)
    doc_tokens = normalize_tokens(doc_text)

    # Print original texts
    print(f"Query: {query}")
    print(f"Doc text: {doc_text}")
    
    print(f"Query tokens: {query_tokens}")
    print(f"Doc tokens: {doc_tokens}")

    return len(query_tokens & doc_tokens) / max(len(query_tokens), 1)

# ===========================
# Retrieve relevant context
# ===========================
def retrieve_relevant_context(query, k=5):
    query_emb = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_emb, k)

    retrieved = []
    for idx, dist in zip(indices[0], distances[0]):
        i = int(idx)
        if i < len(metadata["texts"]):
            doc_text = metadata["texts"][i]
            similarity = 1 / (1 + dist)
            overlap_score = token_overlap(query, doc_text)
            # Hybrid score: 70% semantic, 30% token overlap
            # Ensure native Python floats for JSON serialization
            similarity_f = float(similarity)
            overlap_f = float(overlap_score)
            final_score = float(0.7 * similarity_f + 0.3 * overlap_f)

            retrieved.append({
                "text": str(doc_text),
                "Disease": str(metadata["diseases"][i]),
                "Department": str(metadata["departments"][i]),
                "similarity": similarity_f,
                "overlap": overlap_f,
                "final_score": final_score
            })

    retrieved = sorted(retrieved, key=lambda x: x["final_score"], reverse=True)
    return retrieved[:k]

def format_context(docs):
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(
            f"{i}. HastalÄ±k: {doc['Disease']}\n"
            f"BÃ¶lÃ¼m: {doc['Department']}\n"
            f"Belirtiler: {doc['text']}\n"
            f"Relevance (semantic similarity): {doc['similarity']:.3f}, "
            f"Token overlap: {doc['overlap']:.3f}, "
            f"Final score: {doc['final_score']:.3f}"
        )
    return "\n".join(formatted)

def extract_normalized_symptoms(user_input):
    """
    Extract and normalize symptoms from user input into clean symptom phrases.
    Example: "BaÅŸÄ±m aÄŸrÄ±yor ve midem bulanÄ±yor" -> ["baÅŸ aÄŸrÄ±sÄ±", "mide bulantÄ±sÄ±"]
    """
    # Get lemmatized tokens
    lemmas = get_lemmas(user_input)
    
    print(f"ğŸ” Lemmas from Zemberek: {lemmas}")
    
    # Common symptom patterns in Turkish - expanded with more variations
    symptom_mappings = {
        'baÅŸ aÄŸr': 'baÅŸ aÄŸrÄ±sÄ±',  # More specific match
        'karÄ±n aÄŸr': 'karÄ±n aÄŸrÄ±sÄ±',
        'gÃ¶ÄŸÃ¼s aÄŸr': 'gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ±',
        'sÄ±rt aÄŸr': 'sÄ±rt aÄŸrÄ±sÄ±',
        'boyun aÄŸr': 'boyun aÄŸrÄ±sÄ±',
        'eklem aÄŸr': 'eklem aÄŸrÄ±sÄ±',
        'kas aÄŸr': 'kas aÄŸrÄ±sÄ±',
        'baÅŸ dÃ¶n': 'baÅŸ dÃ¶nmesi',  # Only match full phrase
        'Ä±ÅŸÄ±ÄŸa duyar': 'Ä±ÅŸÄ±ÄŸa duyarlÄ±lÄ±k',
        'Ä±ÅŸÄ±k duyar': 'Ä±ÅŸÄ±ÄŸa duyarlÄ±lÄ±k',
        'sese duyar': 'sese duyarlÄ±lÄ±k',
        'ses duyar': 'sese duyarlÄ±lÄ±k',
        'fotofobi': 'Ä±ÅŸÄ±ÄŸa duyarlÄ±lÄ±k',
        'fonofobi': 'sese duyarlÄ±lÄ±k',
        'mide bulant': 'mide bulantÄ±sÄ±',
        'bulantÄ±': 'bulantÄ±',
        'bulant': 'bulantÄ±',
        'kusma': 'kusma',
        'kus': 'kusma',
        'kusmak': 'kusma',
        'Ã¶ksÃ¼rÃ¼k': 'Ã¶ksÃ¼rÃ¼k',
        'Ã¶ksÃ¼r': 'Ã¶ksÃ¼rÃ¼k',
        'ateÅŸ': 'ateÅŸ',
        'halsiz': 'halsizlik',
        'yorgun': 'yorgunluk',
        'uyku': 'uyku sorunu',
        'karÄ±n': 'karÄ±n aÄŸrÄ±sÄ±',
        'gÃ¶ÄŸÃ¼s': 'gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ±',
        'nefes dar': 'nefes darlÄ±ÄŸÄ±',
        'nefes': 'nefes darlÄ±ÄŸÄ±',
        'Ã¶dem': 'Ã¶dem',
        'ÅŸiÅŸ': 'ÅŸiÅŸlik',
        'kÄ±zarÄ±k': 'kÄ±zarÄ±klÄ±k',
        'kaÅŸÄ±ntÄ±': 'kaÅŸÄ±ntÄ±',
        'ishal': 'ishal',
        'kabÄ±zlÄ±k': 'kabÄ±zlÄ±k',
        'titreme': 'titreme',
        'terle': 'terleme',
        'Ã§arpÄ±ntÄ±': 'Ã§arpÄ±ntÄ±',
        'hÄ±rÄ±ltÄ±': 'hÄ±rÄ±ltÄ±',
        'hapÅŸÄ±r': 'hapÅŸÄ±rma',
        'aÄŸr': 'aÄŸrÄ±',  # Generic pain - add last so specific ones match first
    }
    
    # Extract symptoms based on lemmas and input text
    symptoms = []
    text_lower = user_input.lower()
    lemmas_lower = [l.lower() for l in lemmas]
    
    print(f"ğŸ” Looking for symptoms in: {text_lower}")
    print(f"ğŸ” Lemmas (lowercase): {lemmas_lower}")
    
    for key, symptom_name in symptom_mappings.items():
        # Check if key appears in original text or in any lemma
        found = False
        
        # For multi-word keys (with space), check if both parts exist
        if ' ' in key:
            parts = key.split()
            # Check if all parts appear in text or lemmas
            all_parts_found = True
            for part in parts:
                part_found = False
                if part in text_lower:
                    part_found = True
                else:
                    for lemma in lemmas_lower:
                        if part in lemma:
                            part_found = True
                            break
                if not part_found:
                    all_parts_found = False
                    break
            
            if all_parts_found:
                found = True
                print(f"âœ… Found all parts of '{key}' -> {symptom_name}")
        else:
            # Single-word pattern
            if key in text_lower:
                found = True
                print(f"âœ… Found '{key}' in text -> {symptom_name}")
            else:
                for lemma in lemmas_lower:
                    if key in lemma:
                        found = True
                        print(f"âœ… Found '{key}' in lemma '{lemma}' -> {symptom_name}")
                        break
        
        if found and symptom_name not in symptoms:
            symptoms.append(symptom_name)
    
    # Post-processing: Remove generic "aÄŸrÄ±" if specific pain types exist
    specific_pains = ['baÅŸ aÄŸrÄ±sÄ±', 'karÄ±n aÄŸrÄ±sÄ±', 'gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ±', 'sÄ±rt aÄŸrÄ±sÄ±', 
                      'boyun aÄŸrÄ±sÄ±', 'eklem aÄŸrÄ±sÄ±', 'kas aÄŸrÄ±sÄ±']
    has_specific_pain = any(pain in symptoms for pain in specific_pains)
    if has_specific_pain and 'aÄŸrÄ±' in symptoms:
        symptoms.remove('aÄŸrÄ±')
        print(f"ğŸ”§ Removed generic 'aÄŸrÄ±' because specific pain type exists")
    
    print(f"âœ… Final normalized symptoms: {symptoms}")
    
    # If no specific symptoms found, return the lemmatized tokens as fallback
    if not symptoms:
        symptoms = [lemma.lower() for lemma in lemmas if lemma.lower() not in TURKISH_STOPWORDS]
        print(f"âš ï¸ No mappings found, using lemmas as fallback: {symptoms}")
    
    return symptoms

# ===========================
# Ask GPT-4
# ===========================
def ask_gpt4(user_input):
    # Extract normalized symptoms from user input
    normalized_symptoms = extract_normalized_symptoms(user_input)
    normalized_query = ", ".join(normalized_symptoms)
    
    print(f"\nğŸ” Original input: {user_input}")
    print(f"ğŸ” Normalized symptoms: {normalized_symptoms}")
    print(f"ğŸ” Normalized query: {normalized_query}")
    
    # Use normalized query for retrieval
    retrieved_docs = retrieve_relevant_context(normalized_query, k=5)
    context_text = format_context(retrieved_docs)

    system_prompt = (
        "Sen bir tÄ±bbi NLP sistemisin. "
        "AÅŸaÄŸÄ±daki 'veri tabanÄ± iÃ§eriÄŸi' hastalÄ±k, bÃ¶lÃ¼m ve belirtiler bilgisini iÃ§erir. "
        "KullanÄ±cÄ± TÃ¼rkÃ§e olarak belirtilerini girecektir. "
        "YanÄ±tÄ±nÄ± **mutlaka JSON formatÄ±nda ver** ve baÅŸka hiÃ§bir metin ekleme. "
        "JSON yapÄ±sÄ± ÅŸu ÅŸekilde olmalÄ±dÄ±r: "
        "{"
        "'patient_symptoms': [ ... ], "
        "'departments': [ ... ], "
        "'symptoms_to_ask': [ ... ], "
        "'disease_probabilities': [{ 'disease': 'HastalÄ±k AdÄ±', 'probability': 0.xx }], "
        "'explanation': '...' "
        "}"
        "Kurallar: "
        "1. 'patient_symptoms' alanÄ±nda, normalize edilmiÅŸ kullanÄ±cÄ± belirtilerini listele. "
        "2. EÄŸer belirtiler tek bir departmanla yÃ¼ksek gÃ¼venle eÅŸleÅŸiyorsa, 'departments' listesinde sadece o departmanÄ± ver. "
        "3. EÄŸer belirtiler birden fazla departmanla benzer dÃ¼zeyde eÅŸleÅŸiyorsa, 'departments' listesinde en ilgili departmanlarÄ± ver. "
        "4. 'symptoms_to_ask' alanÄ±nda, hastaya sorulabilecek ek belirtileri listele. **Ã‡OK Ã–NEMLÄ°:** "
        "   - SADECE belirtileri ekle (aÄŸrÄ±, bulantÄ±, Ã¶ksÃ¼rÃ¼k gibi), departman veya hastalÄ±k adÄ± ASLA ekleme. "
        "   - HastanÄ±n GÄ°RMEDÄ°ÄÄ° belirtileri sor. "
        "   - AÄŸÄ±r/ciddi belirtileri (felÃ§, sara nÃ¶beti, bayÄ±lma, bilinÃ§ kaybÄ±, ÅŸok gibi) ASLA sorma Ã§Ã¼nkÃ¼ bu belirtileri yaÅŸayan hasta zaten cevap veremez. "
        "   - Sadece hafif-orta ÅŸiddette, gÃ¼nlÃ¼k yaÅŸamda fark edilebilecek belirtileri sor (baÅŸ aÄŸrÄ±sÄ±, bulantÄ±, halsizlik, Ã¶ksÃ¼rÃ¼k, ateÅŸ gibi). "
        "   - Her belirtiyi kÄ±sa ve net sor (Ã¶rn: 'baÅŸ aÄŸrÄ±sÄ±', 'mide bulantÄ±sÄ±', 'Ä±ÅŸÄ±ÄŸa duyarlÄ±lÄ±k'). Unutma, semptomlar sana verdiÄŸim hastalÄ±k kayÄ±tlarÄ±ndan gelmeli. "
        "   - En fazla 10 belirtiyi listeye ekle, Ã¶nem sÄ±rasÄ±na gÃ¶re. "
        "5. 'disease_probabilities' alanÄ±nda, **verilen context iÃ§inde bulunan TÃœM olasÄ± hastalÄ±klarÄ±** (Ã¶rneÄŸin top-k = 5), "
        "departman eÅŸleÅŸmesinden veya olasÄ±lÄ±k dÃ¼zeyinden baÄŸÄ±msÄ±z ÅŸekilde **tam liste olarak** ver. "
        "Her hastalÄ±k iÃ§in 0.00â€“0.99 aralÄ±ÄŸÄ±nda makul bir olasÄ±lÄ±k deÄŸeri ile doldur (Bunun iÃ§in Final score'larÄ± kullan). "
        "6. 'explanation' alanÄ±nda doktorun okuyacaÄŸÄ± kÄ±sa ama detaylÄ± aÃ§Ä±klama olmalÄ±; her hastalÄ±k iÃ§in hangi ek semptomlarÄ± dikkate almasÄ± gerektiÄŸini belirt. "
        "7. EÄŸer belirtiler context ile eÅŸleÅŸmiyorsa, 'departments' ve 'disease_probabilities' boÅŸ listeler, 'symptoms_to_ask' boÅŸ liste, 'explanation' kÄ±sa uyarÄ± mesajÄ± olsun."
    )

    user_prompt = f"Veri tabanÄ± kayÄ±tlarÄ±:\n{context_text}\n\nKullanÄ±cÄ±nÄ±n belirtileri: {normalized_query}"

    print("\n==================== SYSTEM PROMPT ====================")
    print(system_prompt)
    print("\n==================== USER PROMPT ====================")
    print(user_prompt[:2000])

    response = openai.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
            {
                "role": "user",
                "content": f"HastanÄ±n normalize edilmiÅŸ belirtileri: {normalized_symptoms}"
            },
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content, retrieved_docs, normalized_symptoms

# ===========================
# Run example
# ===========================
if __name__ == "__main__":
    print("\nğŸ¤– RAG-based Disease Prediction System\n")
    user_input = "BaÅŸÄ±m aÄŸrÄ±yor ve midem bulanÄ±yor"

    answer, docs = ask_gpt4(user_input)

    print("\n==================== AI YANITI ====================")
    print(answer)
