# ==============================
# IMPORTS
# ==============================
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ==============================
# STEP 1 â€” Embedding Model (Same as for FAISS)
# ==============================
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

# Load existing FAISS index (created earlier)
db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# Build retriever (retrieve top 5 closest matches)
retriever = db.as_retriever(search_kwargs={"k": 5})

# ==============================
# STEP 2 â€” LLM Setup
# ==============================
# Use an instruction-tuned multilingual model (supports Turkish)
# You can use HuggingFace Inference API or local model if downloaded
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",  # Strong open model
    temperature=0.2,
    max_new_tokens=300
)

# ==============================
# STEP 3 â€” Prompt Template (Domain Specific)
# ==============================
template = """
Sen bir tÄ±bbi asistan sistemisin. AÅŸaÄŸÄ±daki belirtilere gÃ¶re olasÄ± hastalÄ±klarÄ±
ve ilgili hastane bÃ¶lÃ¼mlerini Ã¶ner.

Context (bilgi tabanÄ±):
{context}

Soru:
{question}

YanÄ±t:
LÃ¼tfen aÅŸaÄŸÄ±daki formatta cevap ver:
- OlasÄ± hastalÄ±klar ve tahmini olasÄ±lÄ±klarÄ± (%)
- Ã–nerilen hastane bÃ¶lÃ¼mÃ¼
- KÄ±sa aÃ§Ä±klama (neden bu sonucu Ã¶nerdiÄŸin)

YanÄ±t TÃ¼rkÃ§e olmalÄ±dÄ±r.
"""

prompt = ChatPromptTemplate.from_template(template)

# ==============================
# STEP 4 â€” RAG Chain
# ==============================
def format_docs(docs):
    """Combine retrieved documents into a single context string."""
    return "\n".join(doc.page_content for doc in docs)

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ==============================
# STEP 5 â€” Example Query
# ==============================
query = "BaÅŸÄ±m aÄŸrÄ±yor ve midem bulanÄ±yor, hangi bÃ¶lÃ¼me gitmeliyim?"
response = chain.invoke(query)

print("\nğŸ©º Model YanÄ±tÄ±:\n", response)

# ==============================
# STEP 6 â€” Optional Debug Output (Retrieve Top Docs)
# ==============================
docs = retriever.invoke(query)
print("\nğŸ“š En Benzer KayÄ±tlar (FAISS'den):")
for i, doc in enumerate(docs[:3]):
    print(f"\n{i+1}. {doc.page_content[:180]}...")
